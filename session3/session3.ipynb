{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des données avec pandas\n",
    "\n",
    "- types **NumPy**\n",
    "- renommer des colonnes : `df.rename(columns={...))`\n",
    "- trouver/supprimer les données dupliquées : vdf.duplicated()` / `df.drop_duplicates()`\n",
    "- trouver les NaN : `df.isna()` / `df.notna()` / `df.dropna()`\n",
    "- travail sur les chaînes : `series.str.extract()`, `series.str.contains()`, `series.get_dummies()`\n",
    "- mapping : `series.map()`\n",
    "- changer le type d'une série (cast) : `df.astype(type)` / `pd.to_numeric()` / `pd.to_datetime()`\n",
    "- remplacer n'importe quelle valeur : `df.replace({...})`\n",
    "- remplacer les NaN : `df.fillna()`, `series.combine_first()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement et analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('people.csv')\n",
    "df0 = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention** : **pandas** utilise par défaut les types numériques les plus gourmands en mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subtype in [\"int8\", \"int16\", \"int32\", \"int64\"]:\n",
    "    print(np.iinfo(subtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subtype in [\"uint8\", \"uint16\", \"uint32\", \"uint64\"]:\n",
    "    print(np.iinfo(subtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subtype in [\"float16\", \"float32\", \"float64\"]:\n",
    "    print(np.finfo(subtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('people.csv', dtype={\"id\":\"uint16\", \"lon\":\"float16\", \"lat\":\"float16\", \"last_seen\":\"float32\"})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('people.csv', usecols=[\"id\", \"lon\", \"lat\", \"last_seen\"])\n",
    "df1.memory_usage(deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('people.csv', usecols=[\"id\", \"lon\", \"lat\", \"last_seen\"],\n",
    "                  dtype={\"id\":\"uint16\", \"lon\":\"float16\", \"lat\":\"float16\", \"last_seen\":\"float32\"})\n",
    "df1.memory_usage(deep=True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renommage de la colonne 'email address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renommer les colonnes\n",
    "df = df.rename(columns={'email address': 'email'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des lignes dupliquées\n",
    "\n",
    "- `duplicated()` : `True` ou `False` selon si une ligne est dupliquée\n",
    "- `drop_duplicates()` : suppression des lignes dupliquées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lignes dupliquées\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toutes les lignes dupliquées\n",
    "df.loc[df.duplicated(keep=False)].sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des lignes dupliquées\n",
    "df = df.drop_duplicates()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc\n",
    "df.drop_duplicates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des données manquantes\n",
    "\n",
    "`numpy.nan` est utilisé dans **pandas** pour représenter des valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not A Number\n",
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type\n",
    "type(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élément super absorbant\n",
    "np.nan + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élément super absorbant\n",
    "np.sqrt(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élément super absorbant\n",
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# au passage, infinis numpy\n",
    "np.NINF, np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf > 1e100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf + 1e100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf == np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf + np.NINF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf + np.inf > np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests sur les données manquantes\n",
    "\n",
    "- `isna()` ou `isnull()`\n",
    "- `notna()` ou `notnull()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne fonctionne pas\n",
    "df.loc[df['first_name']==np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chercher les first_name Nan\n",
    "df.loc[df['first_name'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sur tout le dataframe\n",
    "df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chercher tous les lignes avec au moins un NaN\n",
    "df.loc[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression de toutes les lignes avec un NaN\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer uniquement les lignes dont le first_name NaN \n",
    "df = df.dropna(subset=['first_name'])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout d'une colonne 'full_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'full_name'  = 'first_name last_name'\n",
    "df['full_name'] = df['first_name'] + ' ' + df['last_name']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne 'address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de address\n",
    "df['address'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout des colonnes 'city' et 'country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de city et country à partir de address\n",
    "df[['city', 'country']] = df['address'].str.extract('(.*), (.*)')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nunique : modalités par colonne\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping du genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse du gender\n",
    "df['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse du gender\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# traitement du gender\n",
    "mapping = {'Female': 'F', 'Male': 'M', 'F': 'F', 'M': 'M'}\n",
    "df['gender'] = df['gender'].map(mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaire incomplet\n",
    "mapping0 = {'Female': 'F', 'Male': 'M'}\n",
    "s = df0['gender'].map(mapping0)\n",
    "df0['gender'].count(), s.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement du gender, map() avec une Series\n",
    "mapping = pd.Series({'Female': 'F', 'Male': 'M', 'F': 'F', 'M': 'M'})\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement du gender avec une Series\n",
    "df['gender'] = df['gender'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# au final\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse du genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse gender NaN\n",
    "len(df0.loc[df0['gender'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse prénom avec gender NaN\n",
    "df0.loc[df0['gender'].isna(), 'first_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse gender\n",
    "df0.loc[df0['gender'].isna(), 'first_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter le genre :\n",
    "\n",
    "1. Autocomplétion avec le fichier people.csv (mais très peu de cas)\n",
    "2. Gender API : https://gender-api.com/fr (simple mais API payante si gros volumes + de 500/mois)\n",
    "3. US SSA baby names : https://www.ssa.gov/oact/babynames/limits.html (\"gratuit\", stats à produire, éventuellement affiner par année de naissance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de l'âge\n",
    "\n",
    "`pandas.Series.astype()` : types\n",
    "\n",
    "`pandas.to_numeric()` : data avec gestion des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re.search('[^0-9]', 'toto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de l'âge\n",
    "df.loc[df['age'].astype(str).str.contains('[^0-9\\.]'), 'age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement de l'âge\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traitement des dates\n",
    "\n",
    "\n",
    "`pandas.to_datetime()` : data, gestion des formats et des erreurs\n",
    "\n",
    "`pandas.Series.combine_first()` : équivalent à `fillna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_time = ['registration', 'last_seen']\n",
    "df[cols_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cols_time].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.loc[df0['last_seen'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion des dates\n",
    "df['registration'] = pd.to_datetime(df['registration'])\n",
    "df['last_seen'] = pd.to_datetime(df['last_seen'], unit='s')\n",
    "# si last_seen est NaN, prendre registration\n",
    "df['last_seen'] = df['last_seen'].fillna(df['registration'])\n",
    "# idem\n",
    "df['last_seen'] = df['last_seen'].combine_first(df['registration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### applymap() et apply() pour DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longueur de chaque élément passé en string\n",
    "df.astype(str).applymap(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par défaut, la fonction s'applique aux colonnes => résultat = ligne\n",
    "df.apply(len, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple de ligne\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la fonction s'applique aux lignes => résultat = colonne\n",
    "df.apply(len, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple de colonne\n",
    "df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# échantillon\n",
    "np.random.seed(0)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traitement de 'currency'\n",
    "\n",
    "Produire une nouvelle colonne numérique 'money_eur'.\n",
    "\n",
    "Pour la conversion USD/EUR, on utilise l'API https://api.exchangeratesapi.io/latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://open.er-api.com/v6/latest/EUR')\n",
    "rates = json.loads(response.content)\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['money'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['currency'] = df['money'].str[0].map({'€': 'EUR', '$': 'USD'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates['rates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['money'].str[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction de la currency\n",
    "df['currency'] = df['money'].str[0].map({'€': 'EUR', '$': 'USD', '£': 'GBP'})\n",
    "df['money_eur'] = df['money'].str[1:].str.replace(',', '.')  # extraction des derniers chars + , => .\n",
    "df['money_eur'] = pd.to_numeric(df['money_eur'])  # conversion en nombre\n",
    "\n",
    "# conversion des monnaies en euros\n",
    "df['money_eur'] = df['money_eur'] / df['currency'].map(rates['rates'])\n",
    "#np.random.seed(0)\n",
    "#df.sample(10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['money_eur'] = df['money_eur'].round(2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des emails\n",
    "\n",
    "On va utiliser des regex pour nettoyer les emails mais mieux vaut utiliser une librairie spécialisée. Par exemple, https://github.com/syrusakbary/validate_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email NaN\n",
    "df['email'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des emails absents\n",
    "df = df.dropna(subset=['email'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails avec chars non admis\n",
    "df.loc[df['email'].str.contains('[^A-Za-z0-9_\\-%+.@]'), 'email'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des blancs\n",
    "df['email'] = df['email'].str.strip()\n",
    "df.loc[df['email'].str.contains('[^A-Za-z0-9_\\-%+.@]'), 'email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex pour vérifier les domaines\n",
    "df.loc[~df['email'].str.contains('.+@.+\\.[A-Za-z]{2,}$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails avec noms de domaine invalides\n",
    "df = df.loc[df['email'].str.contains('.+@.+\\.[A-Za-z]{2,}$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails avec aliases (char +)\n",
    "df.loc[df['email'].str.contains('\\+'), 'email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.sub(r'([^+]+)(?:\\+.*)?(@.+)', r'\\1\\2', \"a.gorz+alias@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des aliases (char +)\n",
    "s = df['email'].str.replace(r'([^+]+)(?:\\+.*)?(@.+)', r'\\1\\2', regex=True)\n",
    "s.loc[186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [abc]\n",
    "# [a-zA-Z]\n",
    "# [^abc] : pas a, b ou c\n",
    "# [^0-9] : pas digit\n",
    "# '<[^>]+>' : tag HTML\n",
    "# '<.+>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caractères parenthèses\n",
    "# \\([0-9]+\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parenthèses non capturante\n",
    "# (?:regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back references\n",
    "# (.*)@(.*)  \\1  \\2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppresion des aliases (char +)\n",
    "df['email'] = df['email'].str.replace(r'([^+]+)(?:\\+.*)?(@.+)', r'\\1\\2', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des emails en double, on conserve la première ligne\n",
    "df = df.drop_duplicates(subset=['email'])\n",
    "df.sort_values('email')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne 'preference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de preference\n",
    "df['preference'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de preference\n",
    "df['preference'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modalités de preference\n",
    "s = set()\n",
    "df['preference'].apply(lambda x: s.update(x.split('/')))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout d'un booléen par preference\n",
    "for x in sorted(s):\n",
    "    df[x] = df['preference'].str.contains(x)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autre façon avec get_dummies\n",
    "df['preference'].str.get_dummies(sep='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignation des préférences\n",
    "tab_preference = df['preference'].str.get_dummies(sep='/')\n",
    "df[tab_preference.columns] = tab_preference.astype(bool)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "var = encoder.fit_transform(df[['preference']])\n",
    "var.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(var.toarray(), columns=encoder.get_feature_names())\n",
    "# pd.DataFrame(var.toarray(), columns=encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def clean_people(df):\n",
    "    \n",
    "    # suppression des lignes dupliquées\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # renommer les colonnes\n",
    "    df = df.rename(columns={'email address': 'email'})\n",
    "    \n",
    "    # supprimer uniquement les lignes dont le first_name vaut NaN \n",
    "    df = df.dropna(subset=['first_name'])\n",
    "    \n",
    "    # ajout d'une colonne 'full_name'\n",
    "    df['full_name'] = df['first_name'] + ' ' + df['last_name']\n",
    "\n",
    "    # calcul de city et country à partir de address\n",
    "    df[['city', 'country']] = df['address'].str.extract('(.*), (.*)')\n",
    "\n",
    "    # traitement du gender\n",
    "    mapping = {'Female': 'F', 'Male': 'M'}\n",
    "    df['gender'] = df['gender'].map(mapping)\n",
    " \n",
    "    # traitement de l'âge\n",
    "    df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "    # conversion des dates\n",
    "    df['registration'] = pd.to_datetime(df.registration)\n",
    "    df['last_seen'] = pd.to_datetime(df.last_seen, unit='s')\n",
    "    # si last_seen est NaN, prendre registration\n",
    "    df['last_seen'] = df['last_seen'].fillna(df['registration'])\n",
    "\n",
    "    # récupération des taux de change\n",
    "    response = requests.get('https://open.er-api.com/v6/latest/EUR')\n",
    "    rates = json.loads(response.content)\n",
    "\n",
    "    # extraction de la currency\n",
    "    df['currency'] = df['money'].str[0].map({'€': 'EUR', '$': 'USD'})\n",
    "    df['money_eur'] = df['money'].str[1:].str.replace(',', '.')  # extraction des derniers chars + , => .\n",
    "    df['money_eur'] = pd.to_numeric(df['money_eur'])  # conversion en nombre\n",
    "\n",
    "    # conversion des monnaies en euros\n",
    "    rates['rates']['EUR'] = 1.0  # ajour de EUR pour pouvoir utiliser map()\n",
    "    df['money_eur'] = df['money_eur'] * df['currency'].map(rates['rates'])\n",
    "\n",
    "    # suppression des emails absents\n",
    "    df = df.dropna(subset=['email'])\n",
    "\n",
    "    # suppression des blancs\n",
    "    df['email'] = df['email'].str.strip()\n",
    "\n",
    "    # emails avec noms de domaine valides\n",
    "    df = df.loc[df['email'].str.contains('.+@[A-Za-z0-9_\\-.]+\\.[A-Za-z]{2,}')]\n",
    "    \n",
    "    # extraction des aliases (char +)\n",
    "    df['email'] = df['email'].str.replace(r'([^+]+)(?:\\+.*)?(@.+)', r'\\1\\2', regex=True)\n",
    "\n",
    "    # suppression des emails en double, on conserve la première ligne\n",
    "    df = df.drop_duplicates(subset=['email'])\n",
    "\n",
    "    # assignation des préférences\n",
    "    tab_preference = df['preference'].str.get_dummies(sep='/')\n",
    "    df[tab_preference.columns] = tab_preference.astype(bool)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "import pandas as pd\n",
    "\n",
    "df0 = pd.read_csv('people.csv')\n",
    "print(df0.shape)\n",
    "\n",
    "df = clean_people(df0)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method chaining\n",
    "\n",
    "def clean_people2(df):\n",
    "    \n",
    "    # récupération des taux de change\n",
    "    response = requests.get('https://open.er-api.com/v6/latest/EUR')\n",
    "    rates = json.loads(response.content)\n",
    "    rates['rates']['EUR'] = 1.0  # ajour de EUR pour pouvoir utiliser map()\n",
    "    \n",
    "    df = (df\n",
    "          .drop_duplicates()\n",
    "          .rename(columns={'email address': 'email'})\n",
    "          .dropna(subset=['first_name'])\n",
    "          .assign(full_name=lambda df_: df_.first_name + ' ' + df_.last_name,\n",
    "                  gender=lambda df_: df_.gender.map({'Female': 'F', 'Male': 'M'}),\n",
    "                  age=lambda df_: pd.to_numeric(df_.age, errors='coerce'),\n",
    "                  registration=lambda df_: pd.to_datetime(df.registration),\n",
    "                  last_seen=lambda df_: pd.to_datetime(df.last_seen, unit='s'))\n",
    "          .assign(last_seen=lambda df_: df.last_seen.fillna(df_.registration))\n",
    "          .pipe(lambda df_: df_.assign(**df_.address.str.extract('(?P<city>.*), (?P<country>.*)')))\n",
    "          .assign(currency=lambda df_: df_.money.str[0].map({'€': 'EUR', '$': 'USD'}),\n",
    "                  money_eur=lambda df_: df_.money.str[1:].str.replace(',', '.'))\n",
    "          .assign(money_eur=lambda df_: pd.to_numeric(df_.money_eur) * df_.currency.map(rates['rates']))\n",
    "          .dropna(subset=['email'])\n",
    "          .assign(email=lambda df_: df_.email.str.strip())\n",
    "          .assign(email=lambda df_: df_.email.str.replace(r'([^+]+)(?:\\+.*)?(@.+)', r'\\1\\2', regex=True))\n",
    "          .loc[lambda df_: df_.email.str.contains('.+@[A-Za-z0-9_\\-.]+\\.[A-Za-z]{2,}')]\n",
    "          .drop_duplicates(subset=['email'])\n",
    "          .pipe(lambda df_: df_.assign(**df_.preference.str.get_dummies(sep='/').astype(bool)))\n",
    "         )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "import pandas as pd\n",
    "\n",
    "df0 = pd.read_csv('people.csv')\n",
    "print(df0.shape)\n",
    "\n",
    "df = clean_people2(df0)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des tables\n",
    "\n",
    "Voir : https://pandas.pydata.org/docs/user_guide/style.html\n",
    "\n",
    "Taux de remplissage d'un dataframe en bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('people.csv')\n",
    "\n",
    "(df.notna()\n",
    " .sum()\n",
    " .to_frame()\n",
    " .rename({0: \"completion\"}, axis=1)\n",
    " .style\n",
    " .bar(color='lightgreen')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taux de remplissage d'un dataframe en color map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.notna()\n",
    " .sum()\n",
    " .mul(100/len(df))\n",
    " .to_frame()\n",
    " .rename({0: \"completion\"}, axis=1)\n",
    " .style.background_gradient(cmap=\"RdYlGn\")\n",
    " .format(\"{:.1f}%\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "1. Téléchargez le fichier Excel \"FranceTHD_Open_Data_Observatoire_2017_T2.xlsx\" sur le niveau des débits sur les réseaux d'accès à Internet : ADSL, câble, Fibre FttH (T2 2015 - T2 2017) de la page : https://www.data.gouv.fr/fr/datasets/niveau-des-debits-sur-les-reseaux-dacces-a-internet-adsl-cable-fibre-ftth-t2-2015-t2-2017/\n",
    "\n",
    "2. Chargez avec pd.read_excel() dans un DataFrame le dernier onglet \"2017 T2\" en mesurant le temps avec %%time en première instruction de cellule.\n",
    "\n",
    "3. Modifiez le nom des 4 premières colonnes en : 'code INSEE', 'commune', 'département', 'nb locaux' par exemple.\n",
    "\n",
    "4. Sauvegardez le DataFrame avec pd.to_pickle().\n",
    "\n",
    "5. Rechargez le DataFrame à partir du fichier pickle en mesurant le temps avec %%time en première instruction de cellule et comparez.\n",
    "\n",
    "6. Effectuez une opération de sélection sur les communes : par exemple, les communes qui commencent par \"SAINT\".\n",
    "\n",
    "7. Diagnostiquez le message d'erreur.\n",
    "\n",
    "8. Corrigez le DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_excel('FranceTHD_Open_Data_Observatoire_2017_T2.xlsx',\n",
    "               sheet_name=-1,\n",
    "               header=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Unnamed: 0':'Code INSEE','Unnamed: 1':'Commune','Unnamed: 2':'Département','Unnamed: 3':'nb locaux'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('THD.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_pickle('THD.pkl')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Commune'].str.startswith('Saint')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Commune'].str.startswith('Saint', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Commune'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Commune'].apply(lambda x: isinstance(x, bool))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Commune']==False, 'Commune'] = 'Faux'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Commune'].str.startswith('Saint')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etudier les multiples options de read_csv()\n",
    "\n",
    "En particulier:\n",
    "\n",
    "<pre>\n",
    "pd.read_csv(\n",
    "    <strong>filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]],</strong>\n",
    "    <strong>sep=',',</strong>\n",
    "    delimiter=None,\n",
    "    <strong>header='infer',</strong>\n",
    "    <strong>names=None,</strong>\n",
    "    <strong>index_col=None,</strong>\n",
    "    <strong>usecols=None,</strong>\n",
    "    squeeze=False,\n",
    "    prefix=None,\n",
    "    mangle_dupe_cols=True,\n",
    "    <strong>dtype=None,</strong>\n",
    "    <strong>engine=None,</strong>\n",
    "    <strong>converters=None,</strong>\n",
    "    true_values=None,\n",
    "    false_values=None,\n",
    "    skipinitialspace=False,\n",
    "    <strong>skiprows=None,</strong>\n",
    "    <strong>skipfooter=0,</strong>\n",
    "    <strong>nrows=None,</strong>\n",
    "    <strong>na_values=None,</strong>\n",
    "    <strong>keep_default_na=True,</strong>\n",
    "    na_filter=True,\n",
    "    verbose=False,\n",
    "    skip_blank_lines=True,\n",
    "    <strong>parse_dates=False,</strong>\n",
    "    infer_datetime_format=False,\n",
    "    keep_date_col=False,\n",
    "    date_parser=None,\n",
    "    dayfirst=False,\n",
    "    cache_dates=True,\n",
    "    iterator=False,\n",
    "    <strong>chunksize=None,</strong>\n",
    "    compression='infer',\n",
    "    <strong>thousands=None,</strong>\n",
    "    <strong>decimal='.',</strong>\n",
    "    lineterminator=None,\n",
    "    quotechar='\"',\n",
    "    quoting=0,\n",
    "    doublequote=True,\n",
    "    escapechar=None,\n",
    "    comment=None,\n",
    "    encoding=None,\n",
    "    encoding_errors=None,\n",
    "    dialect=None,\n",
    "    error_bad_lines=True,\n",
    "    warn_bad_lines=True,\n",
    "    on_bad_lines=None,\n",
    "    delim_whitespace=False,\n",
    "    low_memory=True,\n",
    "    memory_map=False,\n",
    "    float_precision=None,\n",
    "    storage_options=None\n",
    ")\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse automatique avec pandas_profiling\n",
    "\n",
    "https://github.com/pandas-profiling/pandas-profiling\n",
    "\n",
    "**ATTENTION, il vaut mieux installer `pandas_profiling` dans un nouvel environnement**\n",
    "\n",
    "<pre>\n",
    "conda create --name profiling\n",
    "\n",
    "activate profiling OU conda activate profiling\n",
    "\n",
    "conda install -c conda-forge pandas-profiling\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling raw people\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "df = pd.read_csv('people.csv')\n",
    "\n",
    "profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "profile.to_file(\"people.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling clean people\n",
    "profile = ProfileReport(clean_people(df), title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "profile.to_file(\"clean_people.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
